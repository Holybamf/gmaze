{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"sc\" is the SparkContext\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "print sc.master\n",
    "print sc.version\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import python stuff\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n",
    "from pyspark.mllib.feature import PCA as PCAmllib\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans as mllibKMeans, KMeansModel as mllibKMeansModel\n",
    "\n",
    "from pyspark import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# Define and test the function to load netcdf ISAS data:\n",
    "#\n",
    "\n",
    "def get_temp_fast(fpath):\n",
    "    import netCDF4\n",
    "    import numpy as np\n",
    "    ds = netCDF4.Dataset(fpath)\n",
    "    #print ds.variables['latitude']\n",
    "    #print ds.variables['TEMP']\n",
    "    print ds.variables['TEMP']\n",
    "    temp = ds.variables['TEMP'][:]\n",
    "    res = []\n",
    "    \n",
    "    lat = ds.variables['latitude'][:]\n",
    "    lon = ds.variables['longitude'][:]\n",
    "    \n",
    "\t# Load global domain:\n",
    "    ilat = np.where((lat > -100) & (lat < 100))[0]\n",
    "    ilon = np.where((lon > -200) & (lon < 200))[0]\n",
    "    print ilat.shape, ilat.min(), ilat.max()\n",
    "    \n",
    "\t# Load first 50 levels of temperature:\n",
    "    temp = ds.variables['TEMP'][0,:50,ilat.min():ilat.max(),ilon.min():ilon.max()]\n",
    "    print \"Extraction shape\", temp.shape\n",
    "    temp = temp.reshape((50, (len(ilat)-1)*(len(ilon)-1))).T\n",
    "    print temp.shape\n",
    "    x, y = np.meshgrid(lon[ilon][:-1], lat[ilat][:-1])\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "    res = [y.ravel(), x.ravel(), temp]\n",
    "    print res[0].shape, res[1].shape, res[2].shape\n",
    "    ds.close()\n",
    "    return res\n",
    "#print len(get_temp_fast('/toto/ISAS_LPO/ANA_ISAS13/field/2012/ISAS13_20121015_fld_TEMP.nc'))\n",
    "\n",
    "test = True\n",
    "if test:\n",
    "    rdd = sc.parallelize(['/toto/ANA_ISAS13/field/2012/ISAS13_20121015_fld_TEMP.nc'])\n",
    "    rdd_res = rdd.map(get_temp_fast)\n",
    "    rdd_temp = rdd_res.flatMap(lambda x: Vectors.dense(x[2]))\n",
    "    print rdd_temp.take(1)[0].shape\n",
    "    print rdd_temp.count()    \n",
    "\n",
    "#\n",
    "# Load 1 year of monthly ISAS13 data\n",
    "# (This should be done only once)\n",
    "# \n",
    "flist = glob.glob('/toto/ISAS_LPO/ANA_ISAS13/field/2012/*TEMP.nc')\n",
    "rdd = sc.parallelize(flist)\n",
    "rdd_res = rdd.map(get_temp_fast) #.cache() # pas assez de memoire...\n",
    "\n",
    "# and save the RDD as a pickle file distributed in hdfs\n",
    "print rdd_res.count()\n",
    "rdd_res.saveAsPickleFile('hdfs://br156-161.ifremer.fr:8020/tmp/venthsalia_hdp/rdd.pkl')\n",
    "\n",
    "\n",
    "#\n",
    "# Ok, reload the data\n",
    "#\n",
    "rdd_loaded = sc.pickleFile('hdfs://br156-161.ifremer.fr:8020/tmp/venthsalia_hdp/rdd.pkl')\n",
    "rdd_loaded = rdd_loaded.cache()\n",
    "rdd_loaded.count()\n",
    "rdd_b = rdd_loaded.flatMap(lambda x:x[2]).map(lambda x: Vectors.dense(x))\n",
    "print rdd_b.count()\n",
    "print rdd_b.take(1)\n",
    "\n",
    "#\n",
    "# Profiles standardisation\n",
    "#\n",
    "new_scalar = StandardScaler(withMean=True, withStd=True).fit(rdd_b)  \n",
    "print type(new_scalar)\n",
    "scaler3 = new_scalar.transform(rdd_b)\n",
    "\n",
    "#\n",
    "# Profiles compression with PCA\n",
    "#\n",
    "model = PCAmllib(10).fit(scaler3)\n",
    "print type(model)\n",
    "transformed = model.transform(scaler3)\n",
    "print type(transformed)\n",
    "print transformed.count()\n",
    "print transformed.first()\n",
    "\n",
    "# \n",
    "# Train a Profiles classification model with KMean\n",
    "#\n",
    "NBCLUSTERS=8\n",
    "INITMODE='kmean||'   # kmean|| or random\n",
    "clusters = mllibKMeans.train(\n",
    "                transformed, \n",
    "                NBCLUSTERS, maxIterations=100, \n",
    "                initializationMode=INITMODE)\n",
    "# Rq: Option \"runs=5\" has been deprecated in 1.6.0\n",
    "\n",
    "#clusters.save(sc, PROJDIR+'/tmp/ModelTest2')\n",
    "#clusters.save(sc, MODEL_SAVE_PATH)\n",
    "\n",
    "dfc = pd.DataFrame(clusters.clusterCenters)\n",
    "print type(clusters)\n",
    "print dfc.head(NBCLUSTERS)\n",
    "\n",
    "#\n",
    "# Classify profiles:\n",
    "#\n",
    "res = clusters.predict(transformed)\n",
    "print type(res)\n",
    "print res.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
